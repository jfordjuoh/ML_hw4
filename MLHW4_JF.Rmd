---
title: "MLHW4_JF"
date: February 11, 2022
output: word_document
---

```{r, echo = FALSE}
library(tidyverse)
library(Amelia)
library(caret)
library(stats)
library(cluster)
library(factoextra)
library(dplyr)
library(readxl)
library(haven)
library(knitr)
library(broom)
```


Q's for sneha:
Qoes gpaq8totmin and gpaq11days have to be a factor? based on the question asked, I believe this is a continuous variable.

For q4 when I run the code in R, I'm getting one for the optimal number of clusters. When I knit it I'm getting 3 as the optimal number of clusters. 
For q4, if I do it one way the optimal number of clusters as 3. If i do it the other way I get the optimal number of clusters as 1. What should I do? also how do I use cbind in the first way ?

# Part I Setup: Data Cleaning, Iniital Linear Regression, and Data Partitioning,   
```{r}
# First Task: Perform basic data cleaning. Note which features are continuous, which are categorical and ensure they are being stored that way in your R dataset (That is, if categorical variables have been read-in as continuous variables, convert them to factors)

class4 = read_csv("class4_p1.csv") %>%
    janitor::clean_names() %>%
    rename(observation = x1) %>% #all the variables are cont so we have to convert them all to factors/provide levels except for healthy days (outcome) because we are doing a linear reg 
    mutate(chronic1 = as.factor(chronic1),
           chronic3 = as.factor(chronic3),
           chronic4 = as.factor(chronic4),
           tobacco1 = as.factor(tobacco1), 
           alcohol1 = as.factor(alcohol1),
           habits5 = as.factor(habits5), 
           habits7 = as.factor(habits7),
           agegroup = as.factor(agegroup),
           dem3 = as.factor(dem3),
           dem4 = as.factor(dem4),
           dem8 = as.factor(dem8),
           povertygroup = as.factor(povertygroup)
           ) %>% 
      drop_na()  #dropping all the missing data. The observations don't start at 1 anymore bc it dropped some people

summary(class4)

missmap(class4, main = "Missing values vs observed")
```


```{r}
# First Task part2: Constructing my linear regression 
model1 = lm(healthydays ~ gpaq8totmin + gpaq11days + bmi, data = class4)
summary(model1)
confint(model1)

model2 = lm(healthydays ~ gpaq8totmin + gpaq11days + bmi + povertygroup + agegroup, data = class4)
summary(model2)
confint(model2)
```


```{r}
# Second Task: Partition data into training and testing (use a 70/30 split)  

#Note to self: First I need to find the correlated predictors and remove them using the cor function and then the find correlation function. Set my cutoff to 0.9.
class4_num = class4 %>% 
  select(where(is.numeric)) #only selected observations,bmi, gpaq8,gpaq11,and healthydays

correlations_class4 <- cor(class4_num, use = "complete.obs")
highcorrelations <- findCorrelation(correlations_class4, cutoff = 0.9) #high.correlations was empty so the data isn't correlated so the following dataset shouldn't have any variables.
class4_lowcorr = class4_num[,-highcorrelations]

#NTS: Now I need to center and scale my data using preprocess and then predict. I'll then set my seed to prevent constant randomization and drop the observation variable.

preprocess = preProcess(class4_num, method = c("center", "scale"))
#Output pre-processed values
transformed_vals <- predict(preprocess, class4_num)

set.seed(1000)
class4$observation <- NULL #copied from JS code

train.index <- createDataPartition(class4$healthydays, p = 0.7, list = FALSE)

class4_train <- class4[train.index,]
class4_test <- class4[-train.index,]

```


# PART I: IMPLEMENTING A SIMPLE PREDICTION PIPELINE
## Question 1 and 2
```{r}
#Q1 Task: Fit two prediction  models using different subsets of the features in the training data. Features can overlap in the two models, but the feature sets should not be exactly the same across models. Clearly state which features were used in the two models.

#Q2 Task: Apply both models within the test data and determine which model is the preferred prediction model using the appropriate evaluation metric(s).

#NTS: If this was a logistic regression I would use a confusion matrix but since this is a linear regression I am using postResample.

#NTS: First I'm tuning my hyperparameter using cross validation (through the train control function.) and doing a 3-fold cross-validation.
control.settings <- trainControl(method = "cv", number = 3)

#Now I'm running my train function
set.seed(1000)
model1_again <- train(healthydays ~ gpaq8totmin + gpaq11days + bmi, data = class4, method = "glm", family = "gaussian", trControl = control.settings)

model1_pred <- predict(model1_again, class4_test)

model1_again

postResample(pred = model1_pred, obs = class4_test$healthydays)


model2_again <- train(healthydays ~ gpaq8totmin + gpaq11days + bmi + povertygroup + agegroup, data = class4, method = "glm", family = "gaussian", trControl = control.settings)

model2_pred <- predict(model2_again, class4_test)

model2_again

postResample(pred = model2_pred, obs = class4_test$healthydays)
```
My first model includes 3 variables: BMI, gpaq8totmin(minutes of total physical activity on home chores on an average day) and gpaq11days (during the last 7 days, on how many days did you walk to get to and from places?). The root mean square error was 7.654. After running the postResample, my RSME became 7.800.

My second model includes these three variables plus poverty group (is your household’s annual income from all source: 1 = <100%, 2 = 100-199%, 3 = 200-399%, 4 = 400-599%, 5 = 600%, 6 = Don’t know) and agegroup (group 1 = 18-24, group 2 = 25-44, group 3 = 45-64, and group 4 = 65+). The root mean square error was 7.444. After running the postResample, my RSME became 7.502.

Since the Mean Square Error (RMSE) is a standard way to measure the error in our model, the larger the number the larger the error. I would recommend the second model which included 5 variables as the preferred final prediction model because it had a slightly lower RMSE. 

## Question 3
```{r}
#Q3 Task: Describe one setting (in 1 or 2 sentences) where the implementation of your final model would be useful.
```
Q3 Answer: My final model may be useful in a low-income community where there may be high prevalence of chronic diseases and researchers would like to understand how predictors like poverty or physical activity levels impact an individuals typical amount of healthy days.


# PART II: CONDUCTING AN UNSUPERVISED ANALYSIS 
Using the dataset from the Group assignment Part 3 (USArrests), identify clusters using hierarchical analysis. Use an agglomerative algorithm for hierarchical clustering. Use a Euclidian distance measure to construct your dissimilarity matrix.

## Question 4
```{r}
#Question IV Task: Conduct a hierarchical clustering analysis. Be sure to specify the linkage method used. Within your analysis, make sure you do both of the following:
#  1 Determine the optimal number of clusters using a clear, data-driven strategy.
#  2 Describe the composition of each cluster in terms of the original input features


#Alternatives for hierarchical clustering
data(USArrests)

clusters_hcut <- hcut(USArrests, k = 3, hc_func = "hclust", hc_method = "complete", hc_metric = "euclidian")

clusters_hcut$size
fviz_dend(clusters_hcut, rect = TRUE)
fviz_cluster(clusters_hcut)

gap_stat <- clusGap(USArrests, FUN = hcut, hc_method = "complete", K.max = 10, B = 5)
fviz_gap_stat(gap_stat)

input.feature.vals <- cbind(USArrests,cluster = clusters_hcut$cluster)

input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean)

```
Q4 Answers: I used Euclidean as my distance metric and complete linkage so that I can use the distance between all of the different data points. After visualizing the gapstat, the optimal number of clusters was 3. In the first cluster there were 16 states, 14 states in the second cluster, and 20 states in the third cluster. In cluster one, the average murder (murder arrests per 100,000) was 11.8, the average assault (assault arrests per 100,000) was 273, the percent urban population was 68.3, and the average rape arrests (rape arrests per 100,000) was 28.4. 

In cluster two, the average murder (murder arrests per 100,000) was 8.21, the average assault (assault arrests per 100,000) was 173, the percent urban population was 70.6, and the average rape arrests (rape arrests per 100,000) was 22.8. 

In cluster three, the average murder (murder arrests per 100,000) was 4.27, the average assault (assault arrests per 100,000) was 87.6, the percent urban population was 59.8, and the average rape arrests (rape arrests per 100,000) was 14.4. 

## Question 5
```{r}
#Question 5 Task: Pretend that the data are from 2020 and not 1973. Describe one research question that can be addressed using the newly identified clusters. Briefly comment on any scientific or ethical considerations one should review before using these clusters for your specific question. NOTE: The clusters can be used as an exposure, an outcome or a covariate.
```
Q5 Answer: Is there a relationship between number of COVID-19 deaths (outcome) and the percent of urban population within the clusters? One onsideration is that within the same cluster, some states may have a higher urban population percentage compared to others, which can bias the results of the analysis. For instance, New York and Alabama are in the same cluster but the two states have a big difference in their urban populations. Also the researchers may want to consider the differences in the access to care/affordability of care in the clusters, as this would be related to whether or not an individual may have received appropriate COVID treatment prior to death. 
